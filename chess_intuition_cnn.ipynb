{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bf8bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from preprocessing import fen_to_piece_maps\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a98baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEVICE == torch.device(\"cpu\"):\n",
    "    print(\"Using CPU, not recommended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e877cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_fens = [example['fen'] for example in batch]\n",
    "    labels = torch.tensor(\n",
    "        [example['target'] for example in batch],\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    inputs = torch.stack([\n",
    "        torch.tensor(fen_to_piece_maps(fen), dtype=torch.float32)\n",
    "        for fen in batch_fens\n",
    "    ])\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = load_from_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_10m/train\"))\n",
    "# val_dataset = load_from_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_10m/validation\"))\n",
    "# test_dataset = load_from_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_10m/test\"))\n",
    "\n",
    "# num_training_examples = len(train_dataset)\n",
    "\n",
    "# train_dataset = train_dataset.to_iterable_dataset(num_shards=32)\n",
    "# val_dataset = val_dataset.to_iterable_dataset()\n",
    "# test_dataset = test_dataset.to_iterable_dataset()\n",
    "\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "# val_dataset = val_dataset.shuffle(buffer_size=10000)\n",
    "# test_dataset = test_dataset.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a43692",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_from_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part1\"))\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_size = int(0.98 * len(dataset))\n",
    "val_size = int(0.01 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Convert to iterable datasets\n",
    "train_dataset = train_dataset.dataset.to_iterable_dataset(num_shards=32)\n",
    "val_dataset = val_dataset.dataset.to_iterable_dataset()\n",
    "test_dataset = test_dataset.dataset.to_iterable_dataset()\n",
    "\n",
    "# Shuffle the datasets\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=10000)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ccc7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e42069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_prob)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Resnet-Like\n",
    "class ChessEvalResNet(nn.Module):\n",
    "    def __init__(self, input_planes=17, channels=128, num_blocks=10, dropout_prob=0.2):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_planes, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(channels, dropout_prob=dropout_prob) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.global_pool(x).view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70193073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(32, 17, 8, 8)  # A batch of 32 chessboard positions\n",
    "\n",
    "model = ChessEvalResNet()\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faecca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "total_iters = NUM_EPOCHS * ((10_000_000 // 256) + 1)\n",
    "\n",
    "model = ChessEvalResNet().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_iters, eta_min=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "steps, train_losses, train_maes, val_losses, val_maes = [], [], [], [], []       # for tracking performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa550541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 0it [00:00, ?it/s]c:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1: 102it [00:10, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 — train_loss: 1.2811, train_mae: 0.4952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 202it [00:19, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200 — train_loss: 0.7552, train_mae: 0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 302it [00:29, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 300 — train_loss: 0.5787, train_mae: 0.3879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 401it [00:38,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400 — train_loss: 0.4905, train_mae: 0.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 498it [00:48,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 500 — train_loss: 0.4366, train_mae: 0.3641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([144])) that is different to the input size (torch.Size([144, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1: 501it [01:57, 11.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating! Step 500 — train_loss: 0.4366, train_mae: 0.3641, val_loss: 0.2184, val_mae: 0.3167, current_LR: 0.0010\n",
      "Saved new best model after 500 iters\n",
      "--------Validation MAE improved to 0.316702--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 602it [02:06, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 600 — train_loss: 0.4005, train_mae: 0.3576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 702it [02:16,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700 — train_loss: 0.3746, train_mae: 0.3527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 802it [02:26, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 800 — train_loss: 0.3554, train_mae: 0.3489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 901it [02:37,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 900 — train_loss: 0.3410, train_mae: 0.3466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 999it [02:47, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 — train_loss: 0.3291, train_mae: 0.3443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1001it [03:55, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating! Step 1000 — train_loss: 0.3291, train_mae: 0.3443, val_loss: 0.2168, val_mae: 0.3102, current_LR: 0.0010\n",
      "Saved new best model after 1000 iters\n",
      "--------Validation MAE improved to 0.310151--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1101it [04:05, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100 — train_loss: 0.3189, train_mae: 0.3419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1201it [04:15,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1200 — train_loss: 0.3106, train_mae: 0.3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1302it [04:25,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1300 — train_loss: 0.3036, train_mae: 0.3386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1401it [04:36,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1400 — train_loss: 0.2974, train_mae: 0.3373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1499it [04:46,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500 — train_loss: 0.2920, train_mae: 0.3361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1501it [05:58, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating! Step 1500 — train_loss: 0.2920, train_mae: 0.3361, val_loss: 0.2161, val_mae: 0.3194, current_LR: 0.0010\n",
      "Saved new best model after 1500 iters\n",
      "--------No significant MAE improvement for 500 iterations--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1600it [06:07, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1600 — train_loss: 0.2875, train_mae: 0.3351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1702it [06:18,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1700 — train_loss: 0.2835, train_mae: 0.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1802it [06:28,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800 — train_loss: 0.2799, train_mae: 0.3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1901it [06:39,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1900 — train_loss: 0.2767, train_mae: 0.3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 1999it [06:49,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000 — train_loss: 0.2742, train_mae: 0.3325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2001it [08:00, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating! Step 2000 — train_loss: 0.2742, train_mae: 0.3325, val_loss: 0.2167, val_mae: 0.3223, current_LR: 0.0010\n",
      "--------No significant MAE improvement for 1000 iterations--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2102it [08:10, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2100 — train_loss: 0.2717, train_mae: 0.3320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2201it [08:20,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2200 — train_loss: 0.2691, train_mae: 0.3314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 2300it [08:30,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2300 — train_loss: 0.2670, train_mae: 0.3309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     18\u001b[0m total_mae \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     21\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:43\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_iter)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_fens \u001b[38;5;241m=\u001b[39m [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m      4\u001b[0m     [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch],\n\u001b[0;32m      5\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m      8\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor(fen_to_piece_maps(fen), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fen \u001b[38;5;129;01min\u001b[39;00m batch_fens\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, labels\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_fens \u001b[38;5;241m=\u001b[39m [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m      4\u001b[0m     [example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch],\n\u001b[0;32m      5\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfen_to_piece_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fen \u001b[38;5;129;01min\u001b[39;00m batch_fens\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs, labels\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_val_mae = float('inf')\n",
    "num_iterations = 0\n",
    "patience_counter = 0    # Count iterations without sufficient improvement\n",
    "PATIENCE = 15000        # How many iterations to wait, noting that total_iters is around 117k for 3 epochs and batch size of 256\n",
    "MIN_IMPROVEMENT = 5e-4  # Minimum improvement required for early stopping to not trigger\n",
    "VAL_ITERS = 500\n",
    "LOG_ITERS = 100\n",
    "\n",
    "early_stop = False\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    if early_stop:\n",
    "        break\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()   \n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        batch_mae = torch.mean(torch.abs(preds - labels)).item()\n",
    "\n",
    "        total_loss += batch_loss * inputs.size(0)\n",
    "        total_mae += batch_mae * inputs.size(0)\n",
    "        num_iterations += 1\n",
    "\n",
    "        # Every 1000 steps, log training loss and MAE\n",
    "        if num_iterations % LOG_ITERS == 0:\n",
    "            avg_train_loss = total_loss / (num_iterations * train_loader.batch_size)\n",
    "            avg_train_mae = total_mae / (num_iterations * train_loader.batch_size)\n",
    "            print(f\"Step {num_iterations} — train_loss: {avg_train_loss:.4f}, train_mae: {avg_train_mae:.4f}\")\n",
    "\n",
    "        # Every 5000 steps, run validation and record performance\n",
    "        if num_iterations % VAL_ITERS == 0:\n",
    "            model.eval()\n",
    "            val_loss_sum = 0.0\n",
    "            val_mae_sum = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(DEVICE), val_labels.to(DEVICE)\n",
    "                    val_pred = model(val_inputs)\n",
    "                    val_loss = criterion(val_pred, val_labels)\n",
    "                    val_mae = torch.mean(torch.abs(val_pred - val_labels))\n",
    "\n",
    "                    val_loss_sum += val_loss.item() * val_inputs.size(0)\n",
    "                    val_mae_sum += val_mae.item() * val_inputs.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss_sum / 250_000\n",
    "            avg_val_mae = val_mae_sum / 250_000\n",
    "\n",
    "            print(f\"Validating! Step {num_iterations} — train_loss: {avg_train_loss:.4f}, train_mae: {avg_train_mae:.4f}, val_loss: {avg_val_loss:.4f}, val_mae: {avg_val_mae:.4f}, current_LR: {scheduler.get_last_lr()[0]:.4f}\")\n",
    "\n",
    "            steps.append(num_iterations)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_maes.append(avg_train_mae)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_maes.append(avg_val_mae)\n",
    "\n",
    "            # Checkpoint best\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(model.state_dict(), \"best_chess_transformer.pth\")\n",
    "                print(f\"Saved new best model after {num_iterations} iters\")\n",
    "            \n",
    "            # Check if validation MAE improved by 0.0005 at least\n",
    "            if avg_val_mae + MIN_IMPROVEMENT < best_val_mae:\n",
    "                best_val_mae = avg_val_mae\n",
    "                patience_counter = 0  # Reset patience\n",
    "                print(f\"--------Validation MAE improved to {best_val_mae:.6f}--------\")\n",
    "            else:\n",
    "                patience_counter += VAL_ITERS  # because we validate after every VAL_ITERS \n",
    "                print(f\"--------No significant MAE improvement for {patience_counter} iterations--------\")\n",
    "\n",
    "            # Early stopping\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"Early stopping at {num_iterations} iterations (no significant MAE improvement)\")\n",
    "                early_stop = True\n",
    "                break\n",
    "\n",
    "            model.train() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
