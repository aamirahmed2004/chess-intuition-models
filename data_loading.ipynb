{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f9037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from preprocessing import add_representations, fen_to_piece_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting the dataset to make it more manageable\n",
    "\n",
    "input_path = \"data/lichess_db_eval.jsonl\"\n",
    "output_part1 = \"data/lichess_db_eval_part1.jsonl\"\n",
    "# output_part2 = \"data/lichess_db_eval_part2.jsonl\"\n",
    "# output_part3 = \"data/lichess_db_eval_part3.jsonl\"\n",
    "\n",
    "n_split = 60_000_000    # take 60 million rows at a time\n",
    "\n",
    "with open(input_path, \"r\") as in_f, \\\n",
    "     open(output_part1, \"w\") as out_f:\n",
    "\n",
    "    for i, line in enumerate(in_f):\n",
    "        if i < n_split: \n",
    "            out_f.write(line)\n",
    "        else: \n",
    "            print(\"Breaking\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e94f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_select_highest_depth_pv_scaled(batch, c_max=1000):\n",
    "    selected = {\"fen\": [], \"target\": []}\n",
    "\n",
    "    for fen, evals in zip(batch[\"fen\"], batch[\"evals\"]):\n",
    "        if not evals:\n",
    "            continue\n",
    "\n",
    "        best_eval = max(evals, key=lambda e: e[\"depth\"])\n",
    "        if not best_eval[\"pvs\"]:\n",
    "            continue\n",
    "\n",
    "        pv = best_eval[\"pvs\"][0]\n",
    "        cp = pv.get(\"cp\")\n",
    "        mate = pv.get(\"mate\")\n",
    "\n",
    "        if mate is not None:\n",
    "            target = 1.0 if mate > 0 else -1.0\n",
    "        elif cp is not None:\n",
    "            target = max(-1.0, min(1.0, cp / c_max))        # essentially, any centipawn evaluation above a 1000 (i.e. 10 pawns worth of) is clamped to + or - 1, equating it to mate\n",
    "        else:\n",
    "            continue  # skip if no usable score\n",
    "\n",
    "        selected[\"fen\"].append(fen)\n",
    "        selected[\"target\"].append(target)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbb416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 60000000 examples [01:28, 676697.58 examples/s] \n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data/lichess_db_eval_part1.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d33ce84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 60000000/60000000 [51:22<00:00, 19466.14 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda batch: batch_select_highest_depth_pv_scaled(batch),\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"evals\"],\n",
    "    new_fingerprint=\"processed_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "150a02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (8/8 shards): 100%|██████████| 60000000/60000000 [00:39<00:00, 1531498.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea22efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 60000001 examples [01:47, 558218.34 examples/s]\n",
      "Map: 100%|██████████| 60000001/60000001 [47:48<00:00, 20916.20 examples/s]  \n",
      "Saving the dataset (8/8 shards): 100%|██████████| 60000000/60000000 [00:40<00:00, 1482589.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_part2 = load_dataset(\"json\", data_files=\"data/lichess_db_eval_part2.jsonl\", split=\"train\")\n",
    "dataset_part2 = dataset_part2.map(\n",
    "    lambda batch: batch_select_highest_depth_pv_scaled(batch),\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"evals\"],\n",
    "    new_fingerprint=\"processed_dataset\"\n",
    ")\n",
    "dataset.save_to_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae76f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 33340784 examples [28:52, 19247.76 examples/s]  \n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\builder.py:1871\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1871\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1872\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_writer.py:628\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m--> 628\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\pyarrow\\ipc.pxi:529\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\pyarrow\\error.pxi:89\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\fsspec\\implementations\\local.py:431\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_part3 \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/lichess_db_eval_part3.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataset_part3 \u001b[38;5;241m=\u001b[39m dataset_part3\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m batch: batch_select_highest_depth_pv_scaled(batch),\n\u001b[0;32m      4\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     new_fingerprint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave_to_disk(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data/lichess_db_eval_part3\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2084\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2095\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\builder.py:925\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    924\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m    926\u001b[0m     dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m    927\u001b[0m     verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m    930\u001b[0m )\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\builder.py:1001\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    997\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split(split_generator, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs)\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1007\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1008\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\builder.py:1742\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1740\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1742\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[0;32m   1743\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[0;32m   1744\u001b[0m     ):\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   1746\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\builder.py:1898\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1896\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[0;32m   1897\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "dataset_part3 = load_dataset(\"json\", data_files=\"data/lichess_db_eval_part3.jsonl\", split=\"train\")\n",
    "dataset_part3 = dataset_part3.map(\n",
    "    batch_select_highest_depth_pv_scaled,\n",
    "    batched=True,\n",
    "    batch_size=64,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"evals\"],\n",
    "    new_fingerprint=\"processed_dataset\"\n",
    ")\n",
    "dataset.save_to_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5b400a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000001\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_part2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43a089f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEN: 7r/1p3k2/p1bPR3/5p2/2B2P1p/8/PP4P1/3K4 b - - | Target: 0.069\n",
      "FEN: 8/4r3/2R2pk1/6pp/3P4/6P1/5K1P/8 b - - | Target: 0.0\n",
      "FEN: 6k1/6p1/8/4K3/4NN2/8/8/8 w - - | Target: 1.0\n",
      "FEN: r1b2rk1/1p2bppp/p1nppn2/q7/2P1P3/N1N5/PP2BPPP/R1BQ1RK1 w - - | Target: 0.026\n",
      "FEN: 6k1/4Rppp/8/8/8/8/5PPP/6K1 w - - | Target: 1.0\n",
      "FEN: 6k1/6p1/6N1/4K3/4N3/8/8/8 b - - | Target: 1.0\n",
      "FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - | Target: 0.015\n",
      "FEN: 8/8/2N2k2/8/1p2p3/p7/K7/8 b - - | Target: 0.0\n",
      "FEN: 8/1r6/2R2pk1/6pp/3P4/6P1/5K1P/8 w - - | Target: 0.0\n",
      "FEN: 1R4k1/3q1pp1/6n1/b2p2Pp/2pP2b1/p1P5/P1BQrPPB/5NK1 b - - | Target: -0.057\n",
      "FEN: 8/5kp1/6N1/4K3/4N3/8/8/8 w - - | Target: 1.0\n",
      "FEN: 1k1r1r2/pbp3pp/1p1q1p2/2p2Q2/4P3/1P1PB3/P1P3PP/4RRK1 w - - | Target: 0.008\n",
      "FEN: 8/3B4/8/p4p1k/5P1p/Pb6/1P4P1/6K1 w - - | Target: 0.676\n",
      "FEN: r2qk2r/3n2p1/1pp1p3/3pPpb1/P2P1nBp/1NB4P/1PP2P2/R3QR1K w kq f6 | Target: 0.007\n",
      "FEN: 1R6/3q1ppk/6n1/b2p2Pp/2pP2b1/p1P5/P1B1rPPB/2Q2NK1 b - - | Target: -0.076\n",
      "FEN: 3r4/1p3k2/p1bPR3/5p2/2B2P1p/8/PP4P1/3K4 w - - | Target: 0.18\n",
      "FEN: 1r2kb1r/pBp2ppp/4pn2/5b2/Q1pq4/6P1/PP1NPP1P/R1B2RK1 b k - | Target: 0.257\n",
      "FEN: 3r4/6k1/2bPR3/pp3p2/2B2P1p/P7/1P3KP1/8 w - - | Target: 0.337\n",
      "FEN: rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - | Target: 0.02\n",
      "FEN: r2k2r1/pppb1p1p/2p5/8/3Bn3/8/PPP2PPP/2KR1B1R b - - | Target: 0.14\n"
     ]
    }
   ],
   "source": [
    "batch = dataset[:20]\n",
    "for fen, target in zip(batch[\"fen\"], batch[\"target\"]):\n",
    "    print(f\"FEN: {fen} | Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08933b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "sample_piece_maps = fen_to_piece_maps(dataset[0][\"fen\"])        # 7r/1p3k2/p1bPR3/5p2/2B2P1p/8/PP4P1/3K4 b - -\n",
    "for row in sample_piece_maps[0]:\n",
    "    print(row[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e6c424",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ee6e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (4/4 shards): 100%|██████████| 30000000/30000000 [00:08<00:00, 3484144.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into two halves\n",
    "half_index = len(dataset) // 2\n",
    "dataset_partial = dataset.select(range(half_index))\n",
    "\n",
    "# Save the first half to the specified folder\n",
    "dataset_partial.save_to_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_partial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a36b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEN: 7r/1p3k2/p1bPR3/5p2/2B2P1p/8/PP4P1/3K4 b - - | Target: 0.069\n",
      "FEN: 8/4r3/2R2pk1/6pp/3P4/6P1/5K1P/8 b - - | Target: 0.0\n",
      "FEN: 6k1/6p1/8/4K3/4NN2/8/8/8 w - - | Target: 1.0\n",
      "FEN: r1b2rk1/1p2bppp/p1nppn2/q7/2P1P3/N1N5/PP2BPPP/R1BQ1RK1 w - - | Target: 0.026\n",
      "FEN: 6k1/4Rppp/8/8/8/8/5PPP/6K1 w - - | Target: 1.0\n",
      "FEN: 6k1/6p1/6N1/4K3/4N3/8/8/8 b - - | Target: 1.0\n",
      "FEN: rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - | Target: 0.015\n",
      "FEN: 8/8/2N2k2/8/1p2p3/p7/K7/8 b - - | Target: 0.0\n",
      "FEN: 8/1r6/2R2pk1/6pp/3P4/6P1/5K1P/8 w - - | Target: 0.0\n",
      "FEN: 1R4k1/3q1pp1/6n1/b2p2Pp/2pP2b1/p1P5/P1BQrPPB/5NK1 b - - | Target: -0.057\n",
      "FEN: 8/5kp1/6N1/4K3/4N3/8/8/8 w - - | Target: 1.0\n",
      "FEN: 1k1r1r2/pbp3pp/1p1q1p2/2p2Q2/4P3/1P1PB3/P1P3PP/4RRK1 w - - | Target: 0.008\n",
      "FEN: 8/3B4/8/p4p1k/5P1p/Pb6/1P4P1/6K1 w - - | Target: 0.676\n",
      "FEN: r2qk2r/3n2p1/1pp1p3/3pPpb1/P2P1nBp/1NB4P/1PP2P2/R3QR1K w kq f6 | Target: 0.007\n",
      "FEN: 1R6/3q1ppk/6n1/b2p2Pp/2pP2b1/p1P5/P1B1rPPB/2Q2NK1 b - - | Target: -0.076\n",
      "FEN: 3r4/1p3k2/p1bPR3/5p2/2B2P1p/8/PP4P1/3K4 w - - | Target: 0.18\n",
      "FEN: 1r2kb1r/pBp2ppp/4pn2/5b2/Q1pq4/6P1/PP1NPP1P/R1B2RK1 b k - | Target: 0.257\n",
      "FEN: 3r4/6k1/2bPR3/pp3p2/2B2P1p/P7/1P3KP1/8 w - - | Target: 0.337\n",
      "FEN: rnbqkbnr/ppp1pppp/8/3p4/3P4/8/PPP1PPPP/RNBQKBNR w KQkq - | Target: 0.02\n",
      "FEN: r2k2r1/pppb1p1p/2p5/8/3Bn3/8/PPP2PPP/2KR1B1R b - - | Target: 0.14\n"
     ]
    }
   ],
   "source": [
    "batch = dataset_partial[:20]\n",
    "for fen, target in zip(batch[\"fen\"], batch[\"target\"]):\n",
    "    print(f\"FEN: {fen} | Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff3ab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):  59%|█████▉    | 35462656/60000000 [58:56<1:18:26, 5212.97 examples/s] "
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(add_representations, batched=True, batch_size=64, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_partial\"))\n",
    "# dataset_part2 = load_from_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000190D1AA4D30>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\syeda\\miniconda3\\envs\\DL\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Map (num_proc=4):   0%|          | 0/60000000 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "dataset_part2 = dataset_part2.map(add_representations, batched=True, batch_size=64, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef808af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part2.save_to_disk(os.path.join(os.getcwd(), \"processed_data/lichess_db_eval_part2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
